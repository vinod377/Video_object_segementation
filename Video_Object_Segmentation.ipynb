{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7153f2f-ae67-44ee-a5f3-e95fa83e6528",
   "metadata": {},
   "source": [
    "# One Shot Video Object Segmentation.\n",
    "The Implementation is based on the paper [https://arxiv.org/abs/1611.05198]. Whole implementation is in tensorflow keras.The Dataset used for training purpose is Davis. The Pipeline followed in the notebook is\n",
    "- Import of module and packages.\n",
    "- Data Processing and analysis.\n",
    "- DataGenerator along with Augmentation function.\n",
    "- Model Creation.\n",
    "- Training Script.\n",
    "- Fine tunning and Testing on videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3695af33-0b5d-4fbb-b103-4d81843aaacb",
   "metadata": {},
   "source": [
    " ## Import of different modules and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7533b822-8733-4777-9c01-2500bf3a5bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, regularizers\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "img_folder_path = \"/Users/tangerine/PycharmProjects/dataset/DAVIS2017/train_data/Train/\"\n",
    "img_annotation_path = \"/Users/tangerine/PycharmProjects/dataset/DAVIS2017/train_data/Train_Annotated/\"\n",
    "weight_path = \"/Users/tangerine/stryker/models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30349218-a0aa-47c3-ae0c-8ed48d67c214",
   "metadata": {},
   "source": [
    "## Data Processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd94b926-af22-43f5-bd75-80e8911842e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_analysis:\n",
    "    def __init__(self,img_folder_path,img_annotation_path):\n",
    "        \n",
    "        self.img_folder_path = img_folder_path   \n",
    "        self.img_annotation_path = img_annotation_path\n",
    "        \n",
    "        self.img_path_train = []     # Training list containing image path\n",
    "        self.target_path_train = []  # Training list containing image mask\n",
    "        self.img_path_val = []       # Validation list containing image path\n",
    "        self.target_path_val = []    # Validation list containing image mask\n",
    "        \n",
    "    def __call__(self,visualize=False):\n",
    "        for roots,dirs,files in os.walk(self.img_folder_path):\n",
    "            for dir in dirs:\n",
    "                dir_path_image = os.path.join(roots,dir)  \n",
    "                dir_path_anno  = os.path.join(self.img_annotation_path,dir)\n",
    "                image_files   = os.listdir(dir_path_image)  # all the files in the current directory\n",
    "                random.shuffle(image_files)                 # random shuffling and splitting in train val \n",
    "                length  = len(image_files)\n",
    "                length_train = int(length*0.85)\n",
    "                for file in image_files[:length_train]:\n",
    "                    img_path = os.path.join(dir_path_image,file)\n",
    "                    annotation_path = os.path.join(dir_path_anno,file[:-4]+'.png')\n",
    "                    self.img_path_train.append(img_path)\n",
    "                    self.target_path_train.append(annotation_path)\n",
    "                \n",
    "                for file in image_files[length_train:]:\n",
    "                    img_path = os.path.join(dir_path_image,file)\n",
    "                    annotation_path = os.path.join(dir_path_anno,file[:-4]+'.png')\n",
    "                    self.img_path_val.append(img_path)\n",
    "                    self.target_path_val.append(annotation_path)\n",
    "                    \n",
    "        if visualize:\n",
    "            visualization()                          \n",
    "                    \n",
    "    def visualization():\n",
    "        pass\n",
    "data = Data_analysis(img_folder_path,img_annotation_path)\n",
    "data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706d5d44-7819-45d8-ae62-e574c598bcaf",
   "metadata": {},
   "source": [
    "## DataGenerator and Augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba6a751-5827-40a8-84f7-d5f1c904f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self,batch_size, input_img_paths, target_img_paths,img_size=(300,300)):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_img_paths)//self.batch_size  # number of steps per epoch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = idx * self.batch_size\n",
    "        batch_inp = self.input_img_paths[i:i+self.batch_size]\n",
    "        batch_target = self.target_img_paths[i:i+self.batch_size]\n",
    "        x = np.zeros((self.batch_size,) + (self.img_size[1],self.img_size[0]) + (3,), dtype=\"float32\")  # batch data\n",
    "        y = np.zeros((self.batch_size,) + (self.img_size[1],self.img_size[0]) + (1,), dtype=\"uint8\")\n",
    "        \n",
    "        for j in range(self.batch_size):\n",
    "            img_path = batch_inp[j]\n",
    "            mask_path = batch_target[j]\n",
    "            \n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img,self.img_size)\n",
    "            \n",
    "            mask = cv2.imread(mask_path,0)\n",
    "            mask = cv2.resize(mask,self.img_size)\n",
    "            mask = np.where(mask>0,1,0).astype('float32')\n",
    "            \n",
    "            img,mask = self.augment(img,mask)\n",
    "            x[j] = img/255.0  # Normalization\n",
    "            y[j] = np.expand_dims(mask,2)               \n",
    "\n",
    "        return x,y\n",
    "    \n",
    "    def augment(self,img,mask):\n",
    "        \n",
    "        flag = np.random.randint(0,2)\n",
    "        if flag == 0:   # horizontal_flip\n",
    "            img = cv2.flip(img,1)\n",
    "            mask = cv2.flip(mask,1)\n",
    "        if flag == 1:  # rotation in range of -15 to 15\n",
    "            height, width = img.shape[:2]\n",
    "            center = (width/2, height/2)\n",
    "            rotate_mat = cv2.getRotationMatrix2D(center=center,angle = 20,scale = 1)\n",
    "            img = cv2.warpAffine(src=img, M=rotate_mat,dsize=(width,height))\n",
    "            mask = cv2.warpAffine(src=mask, M=rotate_mat,dsize=(width,height))\n",
    "\n",
    "        return img,mask\n",
    "    \n",
    "\n",
    "# for x,y in data_gen_train:\n",
    "    # print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d01065-0f67-41f7-b816-ce763d317d6c",
   "metadata": {},
   "source": [
    "## Parent Network and Training.\n",
    "### Parent Network.\n",
    "The Parent Netork consist of Base Network, skip connections and transposed convolutional layer. The Base Network used here is\n",
    "VGG16 which is initialized with imagenet weights. The last Convolutional layer output from each block(starting from block2) before Maxpolling layer of VGG16 are passed to covolutional layer and transposed convolutional layer separately and finally fused forming a skip connections.\n",
    "### Loss Function.\n",
    "The cost function used for training is weighted pixelwise cross entropy to deal with class imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f6436ab-1df6-4d8a-a44d-91d92bf04e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_pixelwise_cross_entropy(label, output):\n",
    "    output = tf.nn.sigmoid(output)\n",
    "    labels_pos = tf.cast(tf.greater(label, 0), tf.float32)\n",
    "    labels_neg = tf.cast(tf.less(label, 1), tf.float32)\n",
    "\n",
    "    num_labels_pos = tf.reduce_sum(labels_pos)\n",
    "    num_labels_neg = tf.reduce_sum(labels_neg)\n",
    "    num_total = num_labels_pos + num_labels_neg\n",
    "\n",
    "    loss_pos = tf.reduce_sum(tf.multiply(labels_pos, tf.math.log(output + 0.00001)))\n",
    "    loss_neg = tf.reduce_sum(tf.multiply(labels_neg, tf.math.log(1 - output + 0.00001)))\n",
    "\n",
    "    final_loss = -num_labels_neg / num_total * loss_pos - num_labels_pos / num_total * loss_neg\n",
    "\n",
    "    return final_loss\n",
    "\n",
    "def vgg16_osvs(input_shape,weight='imagenet'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        weights: either Initialization method available in tensorflow or Imagenet weights\n",
    "        input_shape: Input shape of Image\n",
    "        pooling:\n",
    "    Returns:\n",
    "    \"\"\" \n",
    "    kernel_regularizer=regularizers.l1_l2(l1=1e-3, l2=1e-3)\n",
    "    vgg_arch =[\n",
    "        # block1\n",
    "        [['conv', 64 ],['conv', 64 ],['pool']],\n",
    "        # block2       \n",
    "        [['conv', 128],['conv', 128],['pool']],\n",
    "        #block3\n",
    "        [['conv', 256],['conv', 256],['conv', 256  ],['pool']],\n",
    "        #block4\n",
    "        [['conv', 512],['conv', 512],['conv', 512  ],['pool']],\n",
    "        #block5\n",
    "        [['conv', 512],['conv', 512],['conv', 512  ],['pool']],          \n",
    "    ]\n",
    "    \n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "    _,h,w,_ = tf.shape(img_input)\n",
    "    block_cnt = 0\n",
    "    aux_tensor = []\n",
    "    for block in vgg_arch:\n",
    "        block_cnt +=1\n",
    "        lyr_cnt = 0\n",
    "        for i,lyr in enumerate(block):\n",
    "            lyr_cnt+=1\n",
    "            if lyr[0] == 'conv':\n",
    "                out_ch = lyr[1]\n",
    "                name = f'block{block_cnt}_conv{lyr_cnt}'\n",
    "                if lyr_cnt == 1 and block_cnt == 1:                   \n",
    "                    x = layers.Conv2D(out_ch,(3,3),padding='same',activation='relu',\n",
    "                                      kernel_regularizer=kernel_regularizer,name = name)(img_input)\n",
    "                else:\n",
    "                    x = layers.Conv2D(out_ch,(3,3),padding='same',activation= 'relu',\n",
    "                                      kernel_regularizer=kernel_regularizer,name=name)(x)\n",
    "            \n",
    "            elif lyr[0] == 'pool':\n",
    "                aux_lyr = f'aux_lyr1_{block_cnt}'\n",
    "                aux_lyr = layers.Conv2D(16,(3,3),padding='same',name=aux_lyr)(x)\n",
    "                aux_tensor.append(aux_lyr)\n",
    "                \n",
    "                name = f'block{block_cnt}_pool'\n",
    "                x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=name)(x)\n",
    "\n",
    "    ### Main Output ####\n",
    "    stage = 'transposed_lyr_'\n",
    "\n",
    "\n",
    "    tr_lyr2 = layers.Conv2DTranspose(16,(4,4),strides=2,name = stage+'1')(aux_tensor[1])\n",
    "    tr_lyr2 = tr_lyr2[:,:h,:w,:]\n",
    "    \n",
    "    tr_lyr3 = layers.Conv2DTranspose(16,(8,8),strides=4,name = stage+'2')(aux_tensor[2])\n",
    "    tr_lyr3 = tr_lyr3[:,:h,:w,:]\n",
    "    \n",
    "    tr_lyr4 = layers.Conv2DTranspose(16,(16,16),strides=8,name = stage+'3')(aux_tensor[3])\n",
    "    tr_lyr4 = tr_lyr4[:,:h,:w,:]\n",
    "    \n",
    "    tr_lyr5 = layers.Conv2DTranspose(16,(32,32),strides=16,name = stage+'4')(aux_tensor[4])\n",
    "    tr_lyr5 = tr_lyr5[:,:h,:w,:]\n",
    "\n",
    "    concat = tf.concat([tr_lyr2,tr_lyr3,tr_lyr4,tr_lyr5],axis=3)\n",
    "    \n",
    "    output = layers.Conv2D(1,(1,1))(concat)\n",
    "    # output = tf.nn.sigmoid(output)\n",
    "    \n",
    "    model = Model(inputs = img_input,outputs=output)\n",
    "    \n",
    "    if weight=='imagenet':\n",
    "        model.load_weights(weight_path,by_name=True)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fcf0699-c4f3-498c-8d75-14cad7e6a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parent_training(data_gen_train,dat_gen_test):\n",
    "    \n",
    "    model_save_path = \"/model/parent_model/\"\n",
    "    model_name = \"weights-improvement-{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "    checkpoint_filepath = os.path.join(model_save_path,model_name)\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                        filepath=checkpoint_filepath,\n",
    "                                        save_weights_only=True,\n",
    "                                        monitor='val_loss',\n",
    "                                        mode='min',\n",
    "                                        save_best_only=True)\n",
    "    \n",
    "    parent_model = vgg16_osvs((None,None,3),weight='imagenet')\n",
    "    \n",
    "    loss = weighted_pixelwise_cross_entropy\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    parent_model.compile(optimizer=adam,\n",
    "              loss= loss,        \n",
    "              metrics=['accuracy'])   \n",
    "    \n",
    "    parent_model.fit(data_gen_train,\n",
    "                     epochs=10,verbose=1,\n",
    "                     validation_data=dat_gen_test,\n",
    "                     callbacks=model_checkpoint_callback\n",
    "                     )\n",
    "    \n",
    "batch_size = 16\n",
    "img_size = [300,300]\n",
    "data_gen_train = DataGenerator(batch_size,data.img_path_train,\n",
    "                               data.target_path_train)\n",
    "dat_gen_test  = DataGenerator(batch_size,data.img_path_val,\n",
    "                              data.target_path_val)\n",
    "\n",
    "# parent_training(data_gen_train,dat_gen_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63cc202-6dfe-4bdb-b0e4-49e3cac3b519",
   "metadata": {},
   "source": [
    "## FineTunning and Testing.\n",
    "### Fine Tunning.\n",
    "In The Fine tunning, Parent Network is trained with one or more Frame/Ground truth pair for n number of epochs.The frame used for fine tunning is frame at (t-1) time. The fine tunned model is then used for segmenting subsequent frames in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f3e8291-d100-4e44-984d-053bd07a3532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tunning(fine_tune_gen,weight_parent):\n",
    "    \n",
    "    model_ft = vgg16_osvs((None,None,3),weight=None)\n",
    "    model_ft.load_weights(weight_parent)\n",
    "\n",
    "    loss = weighted_pixelwise_cross_entropy\n",
    "    model_ft.compile(optimizer='adam',\n",
    "                  loss= loss,        \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model_ft.fit(fine_tune_gen,epochs=100,verbose=1\n",
    "             )\n",
    "    model_ft.save('./model/finetunned_model.h5')\n",
    "    \n",
    "    \n",
    "weight_parent = \"/Users/tangerine/Downloads/weights-improvement-73-20985.05.hdf5\"  \n",
    "\n",
    "image_path = [\"/Users/tangerine/PycharmProjects/dataset/DAVIS2017/test_data/image/cows/00000.jpg\"]\n",
    "mask_path = [\"/Users/tangerine/PycharmProjects/dataset/DAVIS2017/test_data/mask/cows/00000.png\"]\n",
    "\n",
    "img_size  = [300,300] # same as parent training\n",
    "batch_size = 1\n",
    "fine_tune_gen = DataGenerator(batch_size,image_path,mask_path)\n",
    "# fine_tunning(fine_tune_gen,weight_parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028082ce-871c-4bb5-b3eb-46b6b960b5d9",
   "metadata": {},
   "source": [
    "### Testing.\n",
    "In testing phase, fine tunned model with (t-1) frame/ground truth pair is used to predict mask for frames at time>=t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa29a450-c6b6-4728-a72b-1a588bd89cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def iou_calc(gt_arr,pred_arr):  \n",
    "    iou_list = []\n",
    "    for ind in range(len(gt_arr)):\n",
    "        gt = gt_arr[ind].flatten()\n",
    "        pred = pred_arr[ind].flatten()\n",
    "        intersection = np.sum(np.multiply(gt,pred))\n",
    "        union = np.add(gt,pred)\n",
    "        union = np.sum(np.where(union>1,1,union))\n",
    "        iou = intersection/union\n",
    "        iou_list.append(iou)\n",
    "    print(np.mean(iou_list))\n",
    "            \n",
    "def testing_seq(fine_tunned_model,image_dir):\n",
    "    pred_mask_arr = []\n",
    "    gt_mask_arr = []\n",
    "    \n",
    "    model_test = vgg16_osvs((None,None,3),weight=None)\n",
    "    model_test.load_weights(fine_tunned_model)\n",
    "    \n",
    "    files = os.listdir(image_dir)\n",
    "    files = sorted(files)\n",
    "    for i,file in enumerate(files):\n",
    "        image_path = os.path.join(image_dir,file)\n",
    "        image  = cv2.imread(image_path)/255.0\n",
    "        h,w,_  = image.shape\n",
    "        image  = np.expand_dims(image,axis=0)\n",
    "        st_time = time.time()\n",
    "        output = tf.nn.sigmoid(model_test.predict(image)).numpy()\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"inference time for {i}. Frame is {end_time-st_time}\") \n",
    "        output = np.where(output>0.5,1,0).reshape(h,w)\n",
    "        pred_mask = np.expand_dims(output,axis =0)\n",
    "        pred_mask_arr.append(pred_mask)\n",
    "              \n",
    "        gt_anno_path = os.path.join(anno_dir,file[:-4]+'.png')\n",
    "        anno = cv2.imread(gt_anno_path,0)\n",
    "        annot = np.where(anno>0,1,0).reshape(h,w).astype('float32')\n",
    "        \n",
    "        anno = np.expand_dims(annot,axis=0)\n",
    "        gt_mask_arr.append(anno)        \n",
    "        \n",
    "    return pred_mask_arr,gt_mask_arr\n",
    "\n",
    "\n",
    "fine_tunned_model = \"./model/finetunned_model.h5\"\n",
    "image_dir = \"/Users/tangerine/PycharmProjects/dataset/DAVIS2017/test_data/image/cows/\"\n",
    "anno_dir = \"/Users/tangerine/PycharmProjects/dataset/DAVIS2017/test_data/mask/cows/\"\n",
    "\n",
    "\n",
    "pred_mask_list,gt_mask_list = testing_seq(fine_tunned_model,image_dir)\n",
    "pred_mask_arr = np.asarray(pred_mask_list).transpose([0,2,3,1])\n",
    "gt_mask_arr = np.asarray(gt_mask_list).transpose([0,2,3,1])\n",
    "iou_calc(gt_mask_arr,pred_mask_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b75fb4c-53b1-4da8-a7e7-3554248ed63c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
