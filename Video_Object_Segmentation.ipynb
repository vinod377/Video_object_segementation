{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7153f2f-ae67-44ee-a5f3-e95fa83e6528",
   "metadata": {},
   "source": [
    "# <font color = 'blue'> One Shot Video Object Segmentation</font>\n",
    "The Implementation is based on the paper [https://arxiv.org/abs/1611.05198]. For base Network VGG16 is used along with skip connections.Whole implementation is in tensorflow keras.The Dataset used for training purpose is Davis. The Pipeline followed in the notebook is\n",
    "- Import of module and packages.\n",
    "- Data Processing and analysis.\n",
    "- DataGenerator along with Augmentation function.\n",
    "- Model Creation.\n",
    "- Training Script.\n",
    "- testing and finetunning on videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3695af33-0b5d-4fbb-b103-4d81843aaacb",
   "metadata": {},
   "source": [
    " ## <font color='blue'>Import of different modules and packages.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7533b822-8733-4777-9c01-2500bf3a5bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow import keras\n",
    "\n",
    "img_folder_path = \"/Users/tangerine/PycharmProjects/dataset/DAVIS2017/Train\"\n",
    "img_annotation_path = \"/Users/tangerine/PycharmProjects/dataset/DAVIS2017/Train_Annotated/\"\n",
    "data = \"/Users/tangerine/PycharmProjects/dataset/DAVIS2017/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30349218-a0aa-47c3-ae0c-8ed48d67c214",
   "metadata": {},
   "source": [
    "## <font color='blue'> Data Processing and analysis.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd94b926-af22-43f5-bd75-80e8911842e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do Data Analysis\n",
    "\n",
    "def data_analysis(img_folder_path,img_annotation_path,visualize = False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    image_files = []\n",
    "    annotation_files = []\n",
    "    cnt = 0\n",
    "    for filename in glob.iglob(img_folder_path + '/**/*.jpg', recursive=True):\n",
    "        image_name = filename.split('/')[-2:]\n",
    "        annotation_file = os.path.join(img_annotation_path,image_name[0],image_name[1][:-4]+'.png') \n",
    "        image = cv2.imread(annotation_file)\n",
    "        image_files.append([filename,annotation_file])\n",
    "        cnt+=1\n",
    "        if cnt==100:\n",
    "            break\n",
    "        \n",
    "    if visualize:\n",
    "        for data in image_files:\n",
    "            # try:\n",
    "            image_org = cv2.imread(data[0])\n",
    "            image_mask = cv2.imread(data[1],0)\n",
    "            mask = np.where(image_mask>0,255,0).astype('float32')\n",
    "            cv2.imshow('image',mask)\n",
    "            cv2.waitKey(100)\n",
    "            cv2.destroyAllWindows()       \n",
    "    return image_files\n",
    "image_data_list = data_analysis(img_folder_path,img_annotation_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706d5d44-7819-45d8-ae62-e574c598bcaf",
   "metadata": {},
   "source": [
    "## <font color='blue'>DataGenerator and Augmentation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba6a751-5827-40a8-84f7-d5f1c904f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self,batch_size, input_img_paths, target_img_paths,img_size=(224,224)):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_img_paths)//self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = idx * self.batch_size\n",
    "        batch_inp = self.input_img_paths[i:i+self.batch_size]\n",
    "        batch_target = self.target_img_paths[i:i+self.batch_size]\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        for j, path in enumerate(batch_inp):\n",
    "            # img = load_img(path, target_size=self.img_size)\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.resize(img,self.img_size)\n",
    "            x[j] = img\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "        for j, path in enumerate(batch_inp):\n",
    "            # img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
    "            img = cv2.imread(path,0)\n",
    "            img = cv2.resize(img,self.img_size)\n",
    "            img = np.where(img>0,1,0).astype('float32')\n",
    "            y[j] = np.expand_dims(img, 2)\n",
    "            # y[j] -= 1\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "# creating input image path lis and image_label_path list for data generator \n",
    "input_img_paths = []\n",
    "target_img_paths = []\n",
    "for val in image_data_list:\n",
    "    input_img_paths.append(val[0])\n",
    "    target_img_paths.append(val[1])\n",
    "\n",
    "data_gen_train = DataGenerator(5,input_img_paths,target_img_paths)\n",
    "\n",
    "# Testing data genrator\n",
    "# for item in data_gen_train:\n",
    "#     print(item.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d01065-0f67-41f7-b816-ce763d317d6c",
   "metadata": {},
   "source": [
    "## <font color='blue'>Model creation.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f6436ab-1df6-4d8a-a44d-91d92bf04e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-14 08:06:28.281293: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "aux_lyr1_2 (Conv2D)             (None, 112, 112, 16) 18448       block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "aux_lyr1_3 (Conv2D)             (None, 56, 56, 16)   36880       block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "aux_lyr1_4 (Conv2D)             (None, 28, 28, 16)   73744       block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "aux_lyr1_5 (Conv2D)             (None, 14, 14, 16)   73744       block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool1 (Conv2DTranspose)  (None, 226, 226, 16) 4112        aux_lyr1_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool2 (Conv2DTranspose)  (None, 228, 228, 16) 16400       aux_lyr1_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool3 (Conv2DTranspose)  (None, 232, 232, 16) 65552       aux_lyr1_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool4 (Conv2DTranspose)  (None, 240, 240, 16) 262160      aux_lyr1_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 224, 224, 16) 0           block5_pool1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_1 (Sli (None, 224, 224, 16) 0           block5_pool2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_2 (Sli (None, 224, 224, 16) 0           block5_pool3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_3 (Sli (None, 224, 224, 16) 0           block5_pool4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 224, 224, 64) 0           tf.__operators__.getitem[0][0]   \n",
      "                                                                 tf.__operators__.getitem_1[0][0] \n",
      "                                                                 tf.__operators__.getitem_2[0][0] \n",
      "                                                                 tf.__operators__.getitem_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "output (Conv2D)                 (None, 224, 224, 1)  65          tf.concat[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 15,265,793\n",
      "Trainable params: 15,265,793\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# loss either pixelwise Binary cross entropy or class balancing pixelwise cross entropy\n",
    "\n",
    "def vgg16Vos(weights='Imagenet',input_shape = None,pooling = None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        weights: either Initialization method available in tensorflow or Imagenet weights\n",
    "        input_shape: Input shape of Image\n",
    "        pooling:\n",
    "    Returns:\n",
    "    \"\"\" \n",
    "    vgg_arch =[\n",
    "        # block1\n",
    "        [['conv', 64 ],['conv', 64 ],['pool']],\n",
    "        # block2       \n",
    "        [['conv', 128],['conv', 128],['pool']],\n",
    "        #block3\n",
    "        [['conv', 256],['conv', 256],['conv', 256  ],['pool']],\n",
    "        #block4\n",
    "        [['conv', 512],['conv', 512],['conv', 512  ],['pool']],\n",
    "        #block5\n",
    "        [['conv', 512],['conv', 512],['conv', 512  ],['pool']],          \n",
    "    ]\n",
    "    \n",
    "    ####. Model Defn ########\n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "    block_cnt = 0\n",
    "    aux_tensor = []\n",
    "    for block in vgg_arch:\n",
    "        block_cnt +=1\n",
    "        lyr_cnt = 0\n",
    "        for i,lyr in enumerate(block):\n",
    "            lyr_cnt+=1\n",
    "            if lyr[0] == 'conv':\n",
    "                out_ch = lyr[1]\n",
    "                name = f'block{block_cnt}_conv{lyr_cnt}'\n",
    "                if lyr_cnt == 1 and block_cnt == 1:                   \n",
    "                    x = layers.Conv2D(out_ch,(3,3),padding='same',name=name)(img_input)\n",
    "                else:\n",
    "                    x = layers.Conv2D(out_ch,(3,3),padding='same',name=name)(x)\n",
    "            \n",
    "            elif lyr[0] == 'pool':\n",
    "                aux_lyr = f'aux_lyr1_{block_cnt}'\n",
    "                aux_lyr = layers.Conv2D(16,(3,3),padding='same',name=aux_lyr)(x)\n",
    "                aux_tensor.append(aux_lyr)\n",
    "                \n",
    "                name = f'block{block_cnt}_pool'\n",
    "                x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=name)(x)\n",
    "\n",
    "    ### Main Output ####\n",
    "    stage = 'transposed_lyr_'\n",
    "    tr_lyr2 = layers.Conv2DTranspose(16,(4,4),strides=2,name=name+'1')(aux_tensor[1])\n",
    "    tr_lyr2 = tr_lyr2[:,:224,:224,:]\n",
    "    \n",
    "    tr_lyr3 = layers.Conv2DTranspose(16,(8,8),strides=4,name = name+'2')(aux_tensor[2])\n",
    "    tr_lyr3 = tr_lyr3[:,:224,:224,:]\n",
    "    \n",
    "    tr_lyr4 = layers.Conv2DTranspose(16,(16,16),strides=8,name=name+'3')(aux_tensor[3])\n",
    "    tr_lyr4 = tr_lyr4[:,:224,:224,:]\n",
    "    \n",
    "    tr_lyr5 = layers.Conv2DTranspose(16,(32,32),strides=16,name=name+'4')(aux_tensor[4])\n",
    "    tr_lyr5 = tr_lyr5[:,:224,:224,:]\n",
    "\n",
    "    concat = tf.concat([tr_lyr2,tr_lyr3,tr_lyr4,tr_lyr5],axis=3,name='concat_layer')\n",
    "    \n",
    "    output = layers.Conv2D(1,(1,1),name='output')(concat)\n",
    "    \n",
    "    model = Model(inputs = img_input,outputs=output)\n",
    "    return model\n",
    "\n",
    "model = vgg16Vos(None,(224,224,3))\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.fit(data_gen_train,epochs=100,verbose=1\n",
    "         # )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d7201-7b30-4f05-bf71-3b4ed2890bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
