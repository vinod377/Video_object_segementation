{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7153f2f-ae67-44ee-a5f3-e95fa83e6528",
   "metadata": {},
   "source": [
    "# <font color = 'blue'> One Shot Video Object Segmentation</font>\n",
    "The Implementation is based on the paper [https://arxiv.org/abs/1611.05198]. For base Network VGG16 is used along with skip connections.Whole implementation is in tensorflow keras.The Dataset used for training purpose is Davis. The Pipeline followed in the notebook is\n",
    "- Import of module and packages.\n",
    "- Data Processing and analysis.\n",
    "- DataGenerator along with Augmentation function.\n",
    "- Model Creation.\n",
    "- Training Script.\n",
    "- Fine tunning and Testing on videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3695af33-0b5d-4fbb-b103-4d81843aaacb",
   "metadata": {},
   "source": [
    " ## <font color='blue'>Import of different modules and packages.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7533b822-8733-4777-9c01-2500bf3a5bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow import keras\n",
    "\n",
    "img_folder_path = \"/Users/tangerine/PycharmProjects/dataset/DAVIS2017/Train\"\n",
    "img_annotation_path = \"/Users/tangerine/PycharmProjects/dataset/DAVIS2017/Train_Annotated/\"\n",
    "data = \"/Users/tangerine/PycharmProjects/dataset/DAVIS2017/\"\n",
    "weight_path = \"/Users/tangerine/stryker/models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30349218-a0aa-47c3-ae0c-8ed48d67c214",
   "metadata": {},
   "source": [
    "## <font color='blue'> Data Processing and analysis.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd94b926-af22-43f5-bd75-80e8911842e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_analysis:\n",
    "    def __init__(self,img_folder_path,img_annotation_path):\n",
    "        \n",
    "        self.img_folder_path = img_folder_path   \n",
    "        self.img_annotation_path = img_annotation_path\n",
    "        \n",
    "        self.img_path_train = []\n",
    "        self.target_path_train = []\n",
    "        self.img_path_val = []\n",
    "        self.target_path_val = []\n",
    "        \n",
    "    def __call__(self,visualize=False):\n",
    "        for roots,dirs,files in os.walk(self.img_folder_path):\n",
    "            for dir in dirs:\n",
    "                dir_path_image = os.path.join(roots,dir)\n",
    "                dir_path_anno  = os.path.join(self.img_annotation_path,dir)\n",
    "                image_files   = os.listdir(dir_path_image)\n",
    "                random.shuffle(image_files)\n",
    "                length  = len(image_files)\n",
    "                length_train = int(length*0.85)\n",
    "                for file in image_files[:length_train]:\n",
    "                    img_path = os.path.join(dir_path_image,file)\n",
    "                    annotation_path = os.path.join(dir_path_anno,file[:-4]+'.png')\n",
    "                    self.img_path_train.append(img_path)\n",
    "                    self.target_path_train.append(annotation_path)\n",
    "                \n",
    "                for file in image_files[length_train:]:\n",
    "                    img_path = os.path.join(dir_path_image,file)\n",
    "                    annotation_path = os.path.join(dir_path_anno,file[:-4]+'.png')\n",
    "                    self.img_path_val.append(img_path)\n",
    "                    self.target_path_val.append(annotation_path)\n",
    "                    \n",
    "        if visualize:\n",
    "            visualization()                          \n",
    "                    \n",
    "    def visualization():\n",
    "        pass\n",
    "data = data_analysis(img_folder_path,img_annotation_path)\n",
    "data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706d5d44-7819-45d8-ae62-e574c598bcaf",
   "metadata": {},
   "source": [
    "## <font color='blue'>DataGenerator and Augmentation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba6a751-5827-40a8-84f7-d5f1c904f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self,batch_size, input_img_paths, target_img_paths,img_size=(300,300)):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_img_paths)//self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = idx * self.batch_size\n",
    "        batch_inp = self.input_img_paths[i:i+self.batch_size]\n",
    "        batch_target = self.target_img_paths[i:i+self.batch_size]\n",
    "        x = np.zeros((self.batch_size,) + (self.img_size[1],self.img_size[0]) + (3,), dtype=\"float32\")\n",
    "        y = np.zeros((self.batch_size,) + (self.img_size[1],self.img_size[0]) + (1,), dtype=\"uint8\")\n",
    "        \n",
    "        for j in range(self.batch_size):\n",
    "            img_path = batch_inp[j]\n",
    "            mask_path = batch_target[j]\n",
    "            \n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img,self.img_size)\n",
    "            \n",
    "            mask = cv2.imread(mask_path,0)\n",
    "            mask = cv2.resize(mask,self.img_size)\n",
    "            mask = np.where(mask>0,1,0).astype('float32')\n",
    "            \n",
    "            img,mask = self.augment(img,mask)\n",
    "            x[j] = img\n",
    "            y[j] = np.expand_dims(mask,2)\n",
    "                \n",
    "\n",
    "        return x,y\n",
    "    \n",
    "    def augment(self,img,mask):\n",
    "        flag = np.random.randint(0,5)\n",
    "        if flag == 0:# horizontal_flip\n",
    "            img = cv2.flip(img,1)\n",
    "            mask = cv2.flip(mask,1)\n",
    "        if flag == 1: # rotation in range of -15 to 15\n",
    "            height, width = img.shape[:2]\n",
    "            center = (width/2, height/2)\n",
    "            rotate_mat = cv2.getRotationMatrix2D(center=center,angle = 20,scale = 1)\n",
    "            img = cv2.warpAffine(src=img, M=rotate_mat,dsize=(width,height))\n",
    "            mask = cv2.warpAffine(src=mask, M=rotate_mat,dsize=(width,height))\n",
    "\n",
    "        return img,mask\n",
    "\n",
    "data_gen_train = DataGenerator(16,data.img_path_train,data.target_path_train)\n",
    "dat_gen_test  = DataGenerator(16,data.img_path_val,data.target_path_val)\n",
    "\n",
    "# for x,y in data_gen_train:\n",
    "    # print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d01065-0f67-41f7-b816-ce763d317d6c",
   "metadata": {},
   "source": [
    "## <font color='blue'>Model creation and Training.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f6436ab-1df6-4d8a-a44d-91d92bf04e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-16 14:26:45.262644: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "def weighted_pixelwise_cross_entropy(label, output):\n",
    "    output = tf.nn.sigmoid(output)\n",
    "    labels_pos = tf.cast(tf.greater(label, 0), tf.float32)\n",
    "    labels_neg = tf.cast(tf.less(label, 1), tf.float32)\n",
    "\n",
    "    num_labels_pos = tf.reduce_sum(labels_pos)\n",
    "    num_labels_neg = tf.reduce_sum(labels_neg)\n",
    "    num_total = num_labels_pos + num_labels_neg\n",
    "\n",
    "    loss_pos = tf.reduce_sum(tf.multiply(labels_pos, tf.math.log(output + 0.00001)))\n",
    "    loss_neg = tf.reduce_sum(tf.multiply(labels_neg, tf.math.log(1 - output + 0.00001)))\n",
    "\n",
    "    final_loss = -num_labels_neg / num_total * loss_pos - num_labels_pos / num_total * loss_neg\n",
    "\n",
    "    return final_loss\n",
    "\n",
    "def VGG16(input_shape,weight='imagenet'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        weights: either Initialization method available in tensorflow or Imagenet weights\n",
    "        input_shape: Input shape of Image\n",
    "        pooling:\n",
    "    Returns:\n",
    "    \"\"\" \n",
    "\n",
    "    vgg_arch =[\n",
    "        # block1\n",
    "        [['conv', 64 ],['conv', 64 ],['pool']],\n",
    "        # block2       \n",
    "        [['conv', 128],['conv', 128],['pool']],\n",
    "        #block3\n",
    "        [['conv', 256],['conv', 256],['conv', 256  ],['pool']],\n",
    "        #block4\n",
    "        [['conv', 512],['conv', 512],['conv', 512  ],['pool']],\n",
    "        #block5\n",
    "        [['conv', 512],['conv', 512],['conv', 512  ],['pool']],          \n",
    "    ]\n",
    "    \n",
    "    img_input = layers.Input(shape=input_shape)\n",
    "    _,h,w,_ = tf.shape(img_input)\n",
    "    block_cnt = 0\n",
    "    aux_tensor = []\n",
    "    for block in vgg_arch:\n",
    "        block_cnt +=1\n",
    "        lyr_cnt = 0\n",
    "        for i,lyr in enumerate(block):\n",
    "            lyr_cnt+=1\n",
    "            if lyr[0] == 'conv':\n",
    "                out_ch = lyr[1]\n",
    "                name = f'block{block_cnt}_conv{lyr_cnt}'\n",
    "                if lyr_cnt == 1 and block_cnt == 1:                   \n",
    "                    x = layers.Conv2D(out_ch,(3,3),padding='same',activation='relu',name=name)(img_input)\n",
    "                    x = layers.BatchNormalization()(x)\n",
    "                else:\n",
    "                    x = layers.Conv2D(out_ch,(3,3),padding='same',activation= 'relu',name=name)(x)\n",
    "                    x = layers.BatchNormalization()(x)\n",
    "\n",
    "            \n",
    "            elif lyr[0] == 'pool':\n",
    "                aux_lyr = f'aux_lyr1_{block_cnt}'\n",
    "                aux_lyr = layers.Conv2D(16,(3,3),padding='same',name=aux_lyr)(x)\n",
    "                aux_tensor.append(aux_lyr)\n",
    "                \n",
    "                name = f'block{block_cnt}_pool'\n",
    "                x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=name)(x)\n",
    "\n",
    "    ### Main Output ####\n",
    "    stage = 'transposed_lyr_'\n",
    "\n",
    "\n",
    "    tr_lyr2 = layers.Conv2DTranspose(16,(4,4),strides=2,name = stage+'1')(aux_tensor[1])\n",
    "    tr_lyr2 = tr_lyr2[:,:h,:w,:]\n",
    "    \n",
    "    tr_lyr3 = layers.Conv2DTranspose(16,(8,8),strides=4,name = stage+'2')(aux_tensor[2])\n",
    "    tr_lyr3 = tr_lyr3[:,:h,:w,:]\n",
    "    \n",
    "    tr_lyr4 = layers.Conv2DTranspose(16,(16,16),strides=8,name = stage+'3')(aux_tensor[3])\n",
    "    tr_lyr4 = tr_lyr4[:,:h,:w,:]\n",
    "    \n",
    "    tr_lyr5 = layers.Conv2DTranspose(16,(32,32),strides=16,name = stage+'4')(aux_tensor[4])\n",
    "    tr_lyr5 = tr_lyr5[:,:h,:w,:]\n",
    "\n",
    "    concat = tf.concat([tr_lyr2,tr_lyr3,tr_lyr4,tr_lyr5],axis=3)\n",
    "    \n",
    "    output = layers.Conv2D(1,(1,1))(concat)\n",
    "    # output = tf.nn.sigmoid(output)\n",
    "    \n",
    "    model = Model(inputs = img_input,outputs=output)\n",
    "    \n",
    "    if weight=='imagenet':\n",
    "        model.load_weights(weight_path,by_name=True)\n",
    "        \n",
    "    return model\n",
    "\n",
    "model = VGG16((None,None,3),weight='imagenet')\n",
    "loss = weighted_pixelwise_cross_entropy\n",
    "model.compile(optimizer='adam',\n",
    "              loss= loss,        \n",
    "              metrics=['accuracy'])\n",
    "# model.fit(data_gen_train,epochs=100,verbose=1,validation_data=dat_gen_test\n",
    "         # )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63cc202-6dfe-4bdb-b0e4-49e3cac3b519",
   "metadata": {},
   "source": [
    "## <font color='blue'> FineTunning and Testing. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f3e8291-d100-4e44-984d-053bd07a3532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetunning():\n",
    "    # to do\n",
    "    pass\n",
    "\n",
    "def testing():\n",
    "    # to do\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa29a450-c6b6-4728-a72b-1a588bd89cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
